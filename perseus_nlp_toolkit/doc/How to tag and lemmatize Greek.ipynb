{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `perseus_nlp_toolkit` include some NLTK/CLTK (or NLTK-like) functionality to work with citable and Capitains-compliant texts. I am mostly testing it with Greek texts from the Perseus DL and the Fist 1k Years of Greek and Latin.\n",
    "\n",
    "In this notebook I will show how to use the modules in this package to:\n",
    "* read a corpus of Greek texts\n",
    "* perform a full morphological analysis\n",
    "* lemmatize the tagged words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perseus_nlp_toolkit import lemmatize\n",
    "from perseus_nlp_toolkit import tagger\n",
    "from perseus_nlp_toolkit import CapitainCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module include a `CapitainCorpusReader` to load and tokenize texts that preserve the citation scheme. This reader extends the NLTK regular corpus reader by adding some special methods to access the canonical citations extracted from the TEI- or EpiDOC-compliant texts. The class has methods to tokenize words and sentences and store the citations along with the tokens.\n",
    "\n",
    "As with any corpus reader, the `CapitainCorpusReader` is constructed with a root to the data, a regexp or a simple path (relative to the root) with the file(s) to read and, optionally, a sentence- and a word-tokenizer. In what follows we use the default: a sentence tokenizer for Greek and the regular word-punct tokenizer.\n",
    "\n",
    "Another parameter that can be passed to the reader is the list of TEI elements to bypass. By default, the reader ignores the content of the `tei:note` element. When you deal with dialogic or dramatic texts, you might want to include the elements with the speaker's label (e.g. `tei:label`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.expanduser(\"~/cltk_data/greek/text/canonical-greekLit-master/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with Plato's *Symposium*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we are reading a dialogic text, we get rid of the label with the speaker's name\n",
    "symp = CapitainCorpusReader(root, \"tlg0059/tlg011/tlg.+grc2\\.xml\", \n",
    "                        ≠    exclude_tags=[\"tei:note\", \"tei:label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cite_sents(fileid)` method gives you access to the sentences in a specific file (passed with the \"fileid\" argument), where each token in each sentence preserves also the canonical citaion extracted from the xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = symp.fileids()[0]\n",
    "cite_sents = symp.cite_sents(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('172', 'καὶ'),\n",
       " ('172', 'γὰρ'),\n",
       " ('172', 'ἐτύγχανον'),\n",
       " ('172', 'πρῴην'),\n",
       " ('172', 'εἰς'),\n",
       " ('172', 'ἄστυ'),\n",
       " ('172', 'οἴκοθεν'),\n",
       " ('172', 'ἀνιὼν'),\n",
       " ('172', 'Φαληρόθεν'),\n",
       " ('172', '·')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_sents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, the *Symposium* contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1041\t sentences\n",
      "- 20311\t tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"- {}\\t sentences\".format(len(cite_sents)))\n",
    "print(\"- {}\\t tokens\".format(len(symp.words())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module `tagger` include a python wrapper around the [MateTool](https://code.google.com/archive/p/mate-tools/) morph tagger. This wrapper works very well with the pre-trained models that scored the highest accuracy in the experiment by [Celano, Crane, and Majidi](https://doi.org/10.1515/opli-2016-0020).\n",
    "\n",
    "The models were tested here curtesy of Giuseppe Celano!\n",
    "\n",
    "The `MateMorphTagger` is instantiated by passing the path to the folder where the mate jar file is stored and the path (relative to the root folder where the jar is) to the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mate_tagger = tagger.MateMorphTagger(os.path.expanduser(\"~/Downloads/MateGreek\"), \n",
    "                                     \"LastmateMorph.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 300 ms, sys: 52 ms, total: 352 ms\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cited_tagged_sents = mate_tagger.tag_cite_sents(cite_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the tagger is pretty fast! It took only 1:40 minute (on my machine) to tag more than 20k words.\n",
    "\n",
    "Let's inspect a couple of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('172', 'δοκῶ', 'v|1|s|p|i|a|-|-|-'),\n",
       " ('172', 'μοι', 'p|-|s|-|-|-|m|d|-'),\n",
       " ('172', 'περὶ', 'r|_'),\n",
       " ('172', 'ὧν', 'p|-|p|-|-|-|n|g|-'),\n",
       " ('172', 'πυνθάνεσθε', 'v|2|p|p|m|e|-|-|-'),\n",
       " ('172', 'οὐκ', 'd|_'),\n",
       " ('172', 'ἀμελέτητος', 'n|-|s|-|-|-|f|g|-'),\n",
       " ('172', 'εἶναι', 'v|-|-|p|n|a|-|-|-'),\n",
       " ('172', '.', 'u|_')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cited_tagged_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems encouraging indeed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pos-tag + word couplet is generally more than enough to disambiguate between lemmata. One of the classes available in the `lemmatizer` module adopts the same approach as [Giuseppe Celano's work]() and uses the couplet to perform a lookup into Morpheus database. (Again, we use Giuseppe's unicode conversion of the Morpheus tabs).\n",
    "\n",
    "The `MorpheusLookupLemmatizer` performs the lookup using a bz2-compressed csv table with all the forms, tag, and lemma combination in Morpheus. The table has little less than 1M lines, but a compressed version takes up only 3.3MB of disk space, so it's not that bad. Probably, ad database would be a more efficient way to look up the forms, but it would require users to install, populate and maintain the db to lemmatize a text. A `pandas` dataframe offers a fast and effective way to look up the forms in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.37 s, sys: 175 ms, total: 2.54 s\n",
      "Wall time: 3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemmatizer = lemmatize.MorpheusLookupLemmatizer(\"../lib/morpheus/morpheus_dataframe.csv.bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a lemmatized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.51 s, sys: 17.2 ms, total: 1.52 s\n",
      "Wall time: 1.53 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('172', 'δοκῶ', 'δοκέω', 'v1spia---'),\n",
       " ('172', 'μοι', '', 'p-s---md-'),\n",
       " ('172', 'περὶ', '', 'r--------'),\n",
       " ('172', 'ὧν', 'ὅς', 'p-p---ng-'),\n",
       " ('172', 'πυνθάνεσθε', 'πυνθάνομαι', 'v2ppme---'),\n",
       " ('172', 'οὐκ', 'οὐ', 'd--------'),\n",
       " ('172', 'ἀμελέτητος', '', 'n-s---fg-'),\n",
       " ('172', 'εἶναι', 'εἰμί', 'v--pna---'),\n",
       " ('172', '.', 'punct', 'u--------')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lemmatizer.lemmatize_sentence(cited_tagged_sents[0], include_cite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class has a `lemmatize_sentences` method that can be used to lemmatize and entire sequence of sentences. But, as we shall see, it is an extremely slow process, so to get a better sense of how long the task is taking to complete we'll monitor the loop with `tqdm` and re-write some code for the iteration in the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1041/1041 [27:53<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "lemm_sents = []\n",
    "for s in tqdm(cited_tagged_sents):\n",
    "    ls = lemmatizer.lemmatize_sentence(s, include_cite=True)\n",
    "    lemm_sents.append(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, it took little less than half an hour to lemmatize 1041 sentences. It's not very good! Also, note that I have already speeded up the process considerably by implementing a memoization using [functools.lru_cache](https://docs.python.org/3/library/functools.html#functools.lru_cache). That obviously brought a sensible gain (on the previous attempt, without caching, I had to stop the process after about 50 minutes and 50% of the sentences to go...).\n",
    "\n",
    "Let us inspect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('176', 'τὸν', '', 'p-s---ma-'),\n",
       " ('176', 'οὖν', 'οὖν', 'g--------'),\n",
       " ('176', 'Ἀριστοφάνη', '', 'n-s---fn-'),\n",
       " ('176', 'εἰπεῖν', 'εἶπον', 'v--ana---'),\n",
       " ('176', ',', 'punct', 'u--------'),\n",
       " ('176', 'τοῦτο', '', 'p-s---na-'),\n",
       " ('176', 'μέντοι', '', 'c--------'),\n",
       " ('176', 'εὖ', 'εὖ', 'd--------'),\n",
       " ('176', 'λέγεις', 'λέγω', 'v2spia---'),\n",
       " ('176', ',', 'punct', 'u--------'),\n",
       " ('176', 'ὦ', '', 'i--------'),\n",
       " ('176', 'Παυσανία', '', 'n-s---mv-'),\n",
       " ('176', ',', 'punct', 'u--------'),\n",
       " ('176', 'τὸ', 'ὁ', 'l-s---na-'),\n",
       " ('176', 'παντὶ', '', 'a-s---md-'),\n",
       " ('176', 'τρόπῳ', 'τρόπος', 'n-s---md-'),\n",
       " ('176', 'παρασκευάσασθαι', 'παρασκευάζω', 'v--anm---'),\n",
       " ('176', 'ῥᾳστώνην', 'ῥᾳστώνη', 'n-s---fa-'),\n",
       " ('176', 'τινὰ', '', 'a-s---fa-'),\n",
       " ('176', 'τῆς', 'ὁ', 'l-s---fg-'),\n",
       " ('176', 'πόσεως', 'πόσις', 'n-s---fg-'),\n",
       " ('176', '·', 'punct', 'u--------')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm_sents[102]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very impressive... Though some mistakes can be corrected in postprocessing.\n",
    "\n",
    "To sum up, the simple MorpheusLookupLemmatizer can be OK for short texts and a very superficial round of lemmatization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
